{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 351.2983712748704\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Define the function to compute MAE\n",
    "def compute_mae(image1, image2):\n",
    "    return np.abs(image1 - image2).mean()\n",
    "\n",
    "# Paths to the folders\n",
    "gt_folder = \"/workdir/carrot/splited_data_15k/test/B\"\n",
    "condition_folder = \"/workdir/carrot/splited_data_15k/test/A\"\n",
    "# pre_folder = \"results/108_CT2PET_UncerBBDM3c/LBBDM-f4/sample_to_eval/200\"\n",
    "\n",
    "dataset_name = 'LBBDMxVq13_15k'\n",
    "\n",
    "pre_folder = \"/workdir/ssd2/nguyent_petct/tiennh/BBDM_folk/results/\" + dataset_name + \"/LBBDM-f4/sample_to_eval/400\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lists to store the computed metrics for each pair\n",
    "# ssim_scores = []\n",
    "# psnr_scores = []\n",
    "\n",
    "mae_scores = []\n",
    "high_mae, high_mae_gts, high_mae_pds, high_mae_conditions = [], [], [], []\n",
    "\n",
    "# Iterate through the files in the ground truth folder\n",
    "for filename in os.listdir(gt_folder):\n",
    "    # Make sure the file is a numpy array\n",
    "    if filename.endswith(\".npy\"):\n",
    "        # Construct the paths for the corresponding ground truth and predicted files\n",
    "        try:\n",
    "            gt_path = os.path.join(gt_folder, filename) \n",
    "            pre_path = os.path.join(pre_folder, filename)\n",
    "        \n",
    "            # Load the images as numpy arrays\n",
    "            gt_img = np.load(gt_path, allow_pickle=True)\n",
    "            pre_img = np.load(pre_path, allow_pickle=True)\n",
    "        except:\n",
    "            continue   \n",
    "        # Preprocess the predicted image\n",
    "        pre_img1 = pre_img.mean(axis=-1) / 32767.0\n",
    "        \n",
    "        # Normalize the ground truth image\n",
    "        gt_img1 = gt_img / 32767.0\n",
    "        # Calculate the SSIM, PSNR, and MAE for this pair\n",
    "        # ssim_score = ssim(pre_img, gt_img, data_range=1)\n",
    "        # psnr_score = psnr(pre_img, gt_img, data_range=1)\n",
    "        mae = compute_mae(pre_img1, gt_img1)\n",
    "    \n",
    "        # Append the scores to the corresponding lists\n",
    "        # ssim_scores.append(ssim_score)\n",
    "        # psnr_scores.append(psnr_score)\n",
    "        mae_scores.append(mae * 32767)\n",
    "\n",
    "        if mae * 32767 > 900  : \n",
    "            high_mae_gts.append(gt_img) \n",
    "            high_mae_pds.append(pre_img)\n",
    "            high_mae_conditions.append(np.load(os.path.join(condition_folder, filename), allow_pickle=True))\n",
    "            high_mae.append(mae * 32767)\n",
    "\n",
    "# Calculate the mean scores over all pairs\n",
    "# mean_ssim = np.mean(ssim_scores)\n",
    "# mean_psnr = np.mean(psnr_scores)\n",
    "mean_mae = np.mean(mae_scores)\n",
    "\n",
    "# Print the mean metrics\n",
    "# print(\"Mean SSIM: {}\".format(mean_ssim))\n",
    "# print(\"Mean PSNR: {}\".format(mean_psnr))\n",
    "print(\"Mean MAE: {}\".format(mean_mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_fig_dir = './fig/' + dataset_name + '/400'\n",
    "# save_fig_dir = './fig/small_mae/' + dataset_name\n",
    "\n",
    "if not os.path.exists(save_fig_dir):\n",
    "    os.makedirs(save_fig_dir)\n",
    "\n",
    "\n",
    "# Define a function to visualize images\n",
    "def visualize_and_save(gt, pre, condition , mae, filename):\n",
    "    plt.figure(figsize=(18, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(condition, cmap='gray')\n",
    "    plt.title('CT')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(gt, cmap='gray')\n",
    "    plt.title('Ground Truth')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pre, cmap='gray')\n",
    "    plt.title('Predicted')\n",
    "\n",
    "    \n",
    "    plt.suptitle(f'MAE: {mae:.2f}')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Visualize images with high MAE\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(high_mae)):\n",
    "    gt_image = high_mae_gts[i]\n",
    "    pre_image = high_mae_pds[i]\n",
    "    mae_value = high_mae[i]\n",
    "    condition_img = high_mae_conditions[i]\n",
    "    filename = os.path.join(save_fig_dir, str(i) + '.png' )\n",
    "    # title = f\"MAE: {mae_value:.2f}\"\n",
    "    visualize_and_save(gt_image, pre_image, condition_img, mae_value, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57.192235168588546, 1096.5176792527561)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(mae_scores), np.max(mae_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/BBDM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82,\n",
       "        81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64,\n",
       "        63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46,\n",
       "        45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28,\n",
       "        27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10,\n",
       "         9,  8,  7,  6,  5,  4,  3,  2,  1,  0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "torch.arange(100-1, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,\n",
       "        60,  65,  70,  75,  80,  85,  90,  95, 100, 105, 111, 116, 121,\n",
       "       126, 131, 136, 141, 146, 151, 156, 161, 166, 171, 176, 181, 186,\n",
       "       191, 196, 201, 206, 211, 216, 222, 227, 232, 237, 242, 247, 252,\n",
       "       257, 262, 267, 272, 277, 282, 287, 292, 297, 302, 307, 312, 317,\n",
       "       322, 327, 333, 338, 343, 348, 353, 358, 363, 368, 373, 378, 383,\n",
       "       388, 393, 398, 403, 408, 413, 418, 423, 428, 433, 438, 444, 449,\n",
       "       454, 459, 464, 469, 474, 479, 484, 489, 494, 499, 504, 509, 514,\n",
       "       519, 524, 529, 534, 539, 544, 549, 555, 560, 565, 570, 575, 580,\n",
       "       585, 590, 595, 600, 605, 610, 615, 620, 625, 630, 635, 640, 645,\n",
       "       650, 655, 660, 666, 671, 676, 681, 686, 691, 696, 701, 706, 711,\n",
       "       716, 721, 726, 731, 736, 741, 746, 751, 756, 761, 766, 771, 777,\n",
       "       782, 787, 792, 797, 802, 807, 812, 817, 822, 827, 832, 837, 842,\n",
       "       847, 852, 857, 862, 867, 872, 877, 882, 888, 893, 898, 903, 908,\n",
       "       913, 918, 923, 928, 933, 938, 943, 948, 953, 958, 963, 968, 973,\n",
       "       978, 983, 988, 993, 999])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midsteps = torch.arange(1000 - 1, 1,\n",
    "                                        step=-((1000 - 1) / (200 - 2))).long()\n",
    "steps = torch.cat((midsteps, torch.Tensor([1, 0]).long()), dim=0)\n",
    "np.flip(steps.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   6,  11,  16,  21,  26,  31,  36,  41,  46,  51,  56,  61,\n",
       "        66,  71,  76,  81,  86,  91,  96, 101, 106, 111, 116, 121, 126,\n",
       "       131, 136, 141, 146, 151, 156, 161, 166, 171, 176, 181, 186, 191,\n",
       "       196, 201, 206, 211, 216, 221, 226, 231, 236, 241, 246, 251, 256,\n",
       "       261, 266, 271, 276, 281, 286, 291, 296, 301, 306, 311, 316, 321,\n",
       "       326, 331, 336, 341, 346, 351, 356, 361, 366, 371, 376, 381, 386,\n",
       "       391, 396, 401, 406, 411, 416, 421, 426, 431, 436, 441, 446, 451,\n",
       "       456, 461, 466, 471, 476, 481, 486, 491, 496, 501, 506, 511, 516,\n",
       "       521, 526, 531, 536, 541, 546, 551, 556, 561, 566, 571, 576, 581,\n",
       "       586, 591, 596, 601, 606, 611, 616, 621, 626, 631, 636, 641, 646,\n",
       "       651, 656, 661, 666, 671, 676, 681, 686, 691, 696, 701, 706, 711,\n",
       "       716, 721, 726, 731, 736, 741, 746, 751, 756, 761, 766, 771, 776,\n",
       "       781, 786, 791, 796, 801, 806, 811, 816, 821, 826, 831, 836, 841,\n",
       "       846, 851, 856, 861, 866, 871, 876, 881, 886, 891, 896, 901, 906,\n",
       "       911, 916, 921, 926, 931, 936, 941, 946, 951, 956, 961, 966, 971,\n",
       "       976, 981, 986, 991, 996])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 5\n",
    "time_steps = np.asarray(list(range(0, 1000, c))) + 1\n",
    "\n",
    "time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   6,  11,  16,  21,  26,  31,  36,  41,  46,  51,  56,  61,\n",
       "        66,  71,  76,  81,  86,  91,  96, 101, 106, 112, 117, 122, 127,\n",
       "       132, 137, 142, 147, 152, 157, 162, 167, 172, 177, 182, 187, 192,\n",
       "       197, 202, 207, 212, 217, 223, 228, 233, 238, 243, 248, 253, 258,\n",
       "       263, 268, 273, 278, 283, 288, 293, 298, 303, 308, 313, 318, 323,\n",
       "       328, 334, 339, 344, 349, 354, 359, 364, 369, 374, 379, 384, 389,\n",
       "       394, 399, 404, 409, 414, 419, 424, 429, 434, 439, 445, 450, 455,\n",
       "       460, 465, 470, 475, 480, 485, 490, 495, 500, 505, 510, 515, 520,\n",
       "       525, 530, 535, 540, 545, 550, 556, 561, 566, 571, 576, 581, 586,\n",
       "       591, 596, 601, 606, 611, 616, 621, 626, 631, 636, 641, 646, 651,\n",
       "       656, 661, 667, 672, 677, 682, 687, 692, 697, 702, 707, 712, 717,\n",
       "       722, 727, 732, 737, 742, 747, 752, 757, 762, 767, 772, 778, 783,\n",
       "       788, 793, 798, 803, 808, 813, 818, 823, 828, 833, 838, 843, 848,\n",
       "       853, 858, 863, 868, 873, 878, 883, 889, 894, 899, 904, 909, 914,\n",
       "       919, 924, 929, 934, 939, 944, 949, 954, 959, 964, 969, 974, 979,\n",
       "       984, 989, 994,   1,   0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
